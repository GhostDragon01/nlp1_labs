{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e21760",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c20469f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567789146a604fdf9afc614e8d204562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/DELL/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_data = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86f55b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess(dataset: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Define a function to preprocess the text\n",
    "    \"\"\"\n",
    "    # convert the text to lowercase and remove <br>\n",
    "    dataset['text'] = dataset['text'].lower().replace('<br /><br />', '')\n",
    "    \n",
    "    # replace punctuation with spaces\n",
    "    new_punc = punctuation.replace('!', '')\n",
    "    dataset['text'] = re.sub('['+new_punc+']', ' ', dataset['text'])\n",
    "    \n",
    "    # Replace multiple with a single space\n",
    "    dataset['text'] = \" \".join(dataset['text'].split())\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "train_data = train_data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef725a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_lexicon_list() -> tuple:\n",
    "    \"\"\"\n",
    "    A function to create the list of positive and negative words from the vader lexicon\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(\"./vader_lexicon.txt\") as file:\n",
    "        lines = file.read().splitlines()\n",
    "\n",
    "    lexicon = {}\n",
    "    for line in lines:\n",
    "        splits = line.split(\"\\t\")\n",
    "        lexicon[splits[0]] = float(splits[1])\n",
    "\n",
    "    positive_lexicon = {}\n",
    "    negative_lexicon = {}\n",
    "\n",
    "    for word, score in lexicon.items():\n",
    "        if score >= 1:\n",
    "            positive_lexicon[word] = score\n",
    "        if score <= -1:\n",
    "            negative_lexicon[word] = score\n",
    "    return positive_lexicon, negative_lexicon\n",
    "\n",
    "\n",
    "def get_words_sentiment(tokens: list) -> tuple:\n",
    "    \"\"\" \n",
    "    A function that take a list of words and return the number of positive and negative words.\n",
    "    \"\"\"\n",
    "    positive_lexicon, negative_lexicon = get_lexicon_list()\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in positive_lexicon:\n",
    "            pos_count += 1\n",
    "        if word in negative_lexicon:\n",
    "            neg_count += 1\n",
    "    return pos_count, neg_count\n",
    "\n",
    "\n",
    "def features(text: str) -> list:\n",
    "    \"\"\"\n",
    "    A function to generate a vector with the features described in the subject.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = text.split(\" \")\n",
    "    neg = 1 if \"no\" in tokens else 0\n",
    "    pronouns = tokens.count(\"i\") + tokens.count(\"you\")\n",
    "    exclamation = 1 if \"!\" in text else 0\n",
    "    log_count = log(len(tokens))\n",
    "    positive_words, negative_words = get_words_sentiment(tokens)\n",
    "    return [neg, pronouns, exclamation, log_count, positive_words, negative_words]\n",
    "\n",
    "\n",
    "def get_array(dataset: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply the feature function to all the dataset.\n",
    "    \"\"\"\n",
    "    return np.array([features(text) for text in dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f8e9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1.791759469228055, 2, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it with the example used in class\n",
    "text = \"i love this good movie !\"\n",
    "features(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462c79c",
   "metadata": {},
   "source": [
    "We get the same result : [0, 1, 1, 1.791759469228055, 2, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89187f65",
   "metadata": {},
   "source": [
    "# Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deaa932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa1501c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the feature function to all the dataset\n",
    "all_points = get_array(train_data[\"text\"])\n",
    "labels = train_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858b368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>exclamation</th>\n",
       "      <th>log_count</th>\n",
       "      <th>positive_words</th>\n",
       "      <th>negative_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.673323</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.429346</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.510860</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.795791</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.730100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neg  pronouns  exclamation  log_count  positive_words  negative_words  \\\n",
       "0  1.0       7.0          0.0   5.673323            10.0             6.0   \n",
       "1  1.0       3.0          0.0   5.429346             6.0             4.0   \n",
       "2  1.0       0.0          0.0   4.510860             3.0             3.0   \n",
       "3  0.0       3.0          0.0   4.795791             5.0             5.0   \n",
       "4  0.0      10.0          1.0   5.730100             5.0            11.0   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation of the dataframe\n",
    "df = pd.DataFrame()\n",
    "names = [\"neg\", \"pronouns\", \"exclamation\", \"log_count\", \"positive_words\", \"negative_words\"]\n",
    "\n",
    "for idx, name in enumerate(names):\n",
    "    df[name] = all_points[:, idx]\n",
    "df[\"label\"] = labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e1fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the training set into training and validation set\n",
    "\n",
    "all_points = torch.tensor(all_points, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_points,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=42,\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    stratify=y_train,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efabf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"A logistic regression implementation\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: the dimension of the input features.\n",
    "            nb_classes: the number of classes to predict.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        output_layer = nn.Sigmoid() if nb_classes == 1 else nn.Softmax()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(input_dim, nb_classes),\n",
    "            output_layer,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: the input tensor.\n",
    "        Returns:\n",
    "            The output of activation function.\n",
    "        \"\"\"\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1506b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf962fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2879, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5869, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5838, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5828, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5823, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5821, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5819, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5818, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5818, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5818, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "CPU times: total: 2.31 s\n",
      "Wall time: 1.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Keeping an eye on the losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "def training_loop(train_losses: list, test_losses: list, weight: int = 0.5) -> tuple:\n",
    "    \"\"\" Take the weight_decay in argument and apply a SGD with it \"\"\"\n",
    "    criterion = nn.BCELoss()  # Binary cross entropy\n",
    "    # Stochastic gradient descent\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=weight)\n",
    "    \n",
    "    n_epochs = 1000\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Setting all gradients to zero.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Sending the whole training set through the model.\n",
    "        predictions = model(X_train)\n",
    "        # Computing the loss.\n",
    "        loss = criterion(predictions, y_train)\n",
    "        train_losses.append(loss.item())\n",
    "        if epoch % 100 == 0:\n",
    "            print(loss)\n",
    "        # Computing the gradients and gradient descent.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # When computing the validation loss, we do not want to update the weights.\n",
    "        # torch.no_grad tells PyTorch to not save the necessary data used for\n",
    "        # gradient descent.\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_valid)\n",
    "            loss = criterion(predictions, y_valid)\n",
    "            test_losses.append(loss)\n",
    "    return train_losses, test_losses\n",
    "            \n",
    "train_losses, test_losses = training_loop(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8e15cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18238102b00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the losses\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label=\"Training loss\")\n",
    "plt.plot(np.arange(len(test_losses)), test_losses, label=\"Test loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20a6b614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7153125 0.71275 0.7014\n"
     ]
    }
   ],
   "source": [
    "# Computing the accuracy for our 3 splits.\n",
    "with torch.no_grad():\n",
    "    p_train = model(X_train)\n",
    "    p_train = np.round(p_train.numpy())\n",
    "    training_accuracy = np.mean(p_train == y_train.numpy())\n",
    "    p_valid = model(X_valid)\n",
    "    p_valid = np.round(p_valid.numpy())\n",
    "    valid_accuracy = np.mean(p_valid == y_valid.numpy())\n",
    "    p_test = model(X_test)\n",
    "    p_test = np.round(p_test.numpy())\n",
    "    test_accuracy = np.mean(p_test == y_test.numpy())\n",
    "print(training_accuracy, valid_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f012ec",
   "metadata": {},
   "source": [
    "### The result of the accuracy of our 3 splits :\n",
    "- Training accuracy : 0.7153125 \n",
    "- Validation accuracy : 0.71275 \n",
    "- Test accuracy : 0.7014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980b7ef",
   "metadata": {},
   "source": [
    "# Which features seems to play most for both classes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4684ca75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0262, -0.0492, -0.0055, -0.0145,  0.1286, -0.1506]]),\n",
       " tensor([-0.0041]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[0].state_dict()[\"weight\"], model.classifier[0].state_dict()[\"bias\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b0971",
   "metadata": {},
   "source": [
    "### We get the following result :\n",
    "(tensor([[-0.0262, -0.0492, -0.0055, -0.0145,  0.1286, -0.1506]]),\n",
    " tensor([-0.0041]))\n",
    " \n",
    "The two features (5 and 6) that seem to play the most for both classes are the number of words in the documents that are either in the positive or negative lexicons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e66f2",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "##  weight_decay variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d6aa5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5817, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5798, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5796, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5795, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5794, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5793, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5793, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5791, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5790, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Weight : 0.01 tensor([[-0.1177, -0.0595, -0.0217, -0.0164,  0.1516, -0.1733]]) Bias :  tensor([-0.0072])\n",
      "tensor(0.5790, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5791, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5791, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5791, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Weight : 0.1 tensor([[-0.1012, -0.0572, -0.0185, -0.0157,  0.1464, -0.1682]]) Bias :  tensor([-0.0065])\n",
      "tensor(0.5792, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5928, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5930, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5930, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5930, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5930, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5930, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5930, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5930, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.5930, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Weight : 2 tensor([[-0.0097, -0.0327, -0.0020, -0.0088,  0.0928, -0.1109]]) Bias :  tensor([-0.0018])\n",
      "tensor(0.5930, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Weight : 5 tensor([[-0.0049, -0.0202, -0.0010, -0.0049,  0.0632, -0.0764]]) Bias :  tensor([-0.0010])\n",
      "tensor(0.6119, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Weight : 10 tensor([[-0.0028, -0.0125, -0.0006, -0.0028,  0.0428, -0.0518]]) Bias :  tensor([-0.0005])\n"
     ]
    }
   ],
   "source": [
    "def weight_variation(weight_list: list):\n",
    "    \"\"\" Take a list of weight_decays and apply the SGD on each element of the list \"\"\"\n",
    "    train_losses, test_losses = [], []\n",
    "    for weight in weight_list:\n",
    "        train_losses, test_losses = training_loop(train_losses, test_losses, weight)\n",
    "        print(f\"Weight : {weight}\", model.classifier[0].state_dict()[\"weight\"], \"Bias : \", model.classifier[0].state_dict()[\"bias\"])\n",
    "        \n",
    "\n",
    "weight_list = [0.01, 0.1, 2, 5, 10]\n",
    "\n",
    "weight_variation(weight_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5debf030",
   "metadata": {},
   "source": [
    "## We use a list of 5 weight_decays and get the following result :\n",
    "\n",
    "- Weight_decay : 0.01  --> tensor([[-0.1177, -0.0595, -0.0217, -0.0164,  0.1516, -0.1733]]) Bias :  tensor([-0.0072])\n",
    "- Weight_decay : 0.1 --> tensor([[-0.1012, -0.0572, -0.0185, -0.0157,  0.1464, -0.1682]]) Bias :  tensor([-0.0065])\n",
    "- Weight_decay : 2 --> tensor([[-0.0097, -0.0327, -0.0020, -0.0088,  0.0928, -0.1109]]) Bias :  tensor([-0.0018])\n",
    "- Weight_decay : 5 --> tensor([[-0.0049, -0.0202, -0.0010, -0.0049,  0.0632, -0.0764]]) Bias :  tensor([-0.0010])\n",
    "- Weight_decay : 10 --> tensor([[-0.0028, -0.0125, -0.0006, -0.0028,  0.0428, -0.0518]]) Bias :  tensor([-0.0005])\n",
    "\n",
    "With low values of the parameter weight_decay, the word 'no'(feature 1) has the most influence on both classes.\n",
    "While the value of the weight_decay is increasing, the influence tends to decrease in favor of the weights positive and negative words (feature 5 and 6). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0da490",
   "metadata": {},
   "source": [
    "# Analysis of two failed predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "485de2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>exclamation</th>\n",
       "      <th>log_count</th>\n",
       "      <th>positive_words</th>\n",
       "      <th>negative_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.429346</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.446737</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neg  pronouns  exclamation  log_count  positive_words  negative_words  \\\n",
       "1  1.0       3.0          0.0   5.429346             6.0             4.0   \n",
       "9  1.0       5.0          0.0   5.446737            10.0             9.0   \n",
       "\n",
       "   label  \n",
       "1      0  \n",
       "9      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for failed predictions\n",
    "failed_predictions = ((p_test == y_test.numpy()) == False).nonzero()\n",
    "two_wrong = list(failed_predictions[0][:2]) # We choose two wrong predictions\n",
    "\n",
    "df.iloc[two_wrong]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de084cd7",
   "metadata": {},
   "source": [
    "We can explain the failure of the model by the fact that the 2 most important features which are the number of positive and negative words in the document have very close values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd6c91",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a97e1b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7042"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, np.ravel(y_train))\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779ff10",
   "metadata": {},
   "source": [
    "### We get the following score as result : 0.7042\n",
    "\n",
    "Using the scikit-learn implementation of logistic regression classifier, we can see there is no improvement in the accuracy score. We can then deduce that this is not an implementation problem but a problem of choice and representation of the differents features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
