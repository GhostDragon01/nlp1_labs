{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c075113a",
   "metadata": {},
   "source": [
    "# FastText \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91058352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djulo/.local/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93dcfcb",
   "metadata": {},
   "source": [
    "### Dataset and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f557a30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 399.86it/s]\n",
      "Loading cached split indices for dataset at /home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-5f37fd0866e4f89f.arrow and /home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-dd5732a0e6ac784c.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].train_test_split(\n",
    "    stratify_by_column=\"label\", test_size=0.2, seed=42\n",
    ")\n",
    "test_df = dataset[\"test\"]\n",
    "train_df = train_dataset[\"train\"]\n",
    "valid_df = train_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d987de",
   "metadata": {},
   "source": [
    "### 1. Turn the dataset into a dataset compatible with Fastext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66edbff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-1fd51be787e9806b.arrow\n",
      "Loading cached processed dataset at /home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-40be9da7ef59e51c.arrow\n",
      "Loading cached processed dataset at /home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-64045981f0c2816e.arrow\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def preprocess(dataset: dict) -> dict:\n",
    "    '''Take a dataset, return the dataset preprocessed'''\n",
    "    \n",
    "    # Convert the text to lowercase and remove <br>\n",
    "    dataset['text'] = dataset['text'].lower().replace('<br /><br />', '')\n",
    "    \n",
    "    # Replace punctuation with spaces\n",
    "    dataset['text'] = re.sub('['+punctuation+']', ' ', dataset['text'])\n",
    "    \n",
    "    # Replace multiple with a single space\n",
    "    dataset['text'] = \" \".join(dataset['text'].split())\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Apply the preprocessing function to the train dataset\n",
    "train_df = train_df.map(preprocess)\n",
    "\n",
    "# Apply the preprocessing function to the test dataset\n",
    "test_df = test_df.map(preprocess)\n",
    "\n",
    "# Apply the preprocessing function to the validation dataset\n",
    "valid_df = valid_df.map(preprocess)\n",
    "\n",
    "# Create a training file compatible with fastext\n",
    "with open('training_data.txt', 'w') as file:\n",
    "    # Loop through each string in the list\n",
    "    for data in train_df:\n",
    "        # Write the string to the file and append a newline character\n",
    "        file.write('__label__' + str(data['label']) + ' ' + data['text'] + '\\n')\n",
    "\n",
    "# Create a test file compatible with fastext\n",
    "with open('test_data.txt', 'w') as file:\n",
    "    # Loop through each string in the list\n",
    "    for data in test_df:\n",
    "        # Write the string to the file and append a newline character\n",
    "        file.write('__label__' + str(data['label']) + ' ' + data['text'] + '\\n')\n",
    "        \n",
    "# Create a valid file compatible with fastext\n",
    "with open('valid_data.txt', 'w') as file:\n",
    "    # Loop through each string in the list\n",
    "    for data in valid_df:\n",
    "        # Write the string to the file and append a newline character\n",
    "        file.write('__label__' + str(data['label']) + ' ' + data['text'] + '\\n')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c57afc",
   "metadata": {},
   "source": [
    "### 2. Train a FastText classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d184df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 4M words\n",
      "Number of words:  69701\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2663497 lr:  0.000000 avg.loss:  0.432575 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Train a FastText model\n",
    "fastText_model = fasttext.train_supervised('training_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0047c2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 0.8706, 0.8706)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastText_model.test('validation_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c692e04d",
   "metadata": {},
   "source": [
    "The tuple tells us:\n",
    "1. The number of sample: 25000\n",
    "2. The precision at one: 0.87908\n",
    "3. the recall at one: 0.87908"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc459f65",
   "metadata": {},
   "source": [
    "### 3. Use of the hyperparameters search functionality of FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e6af5",
   "metadata": {},
   "source": [
    "The data in the training_data.txt is already shuffled. We can verify it by checking the first lines of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32f8cd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.895080 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 4M words\n",
      "Number of words:  69701\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  909374 lr:  0.000000 avg.loss:  0.057352 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised('training_data.txt', autotuneValidationFile='validation_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb9e37a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 0.89492, 0.89492)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('validation_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b5cf1",
   "metadata": {},
   "source": [
    "### 4. differences between the default model and the attributes found with hyperparameters search\n",
    "\n",
    "As we have tuned hyperparameters using the autotuneValidationFile option, we will only compare the hyper parameters attributes of the 2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61b15ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model:\n",
      "Value of the learning rate: 0.1\n",
      "Dimensionality of word vectors: 100\n",
      "Number of epochs: 5\n",
      "Max length of word ngram : 1\n",
      "Min length of char ngram: 0\n",
      "Max length of char ngram: 0\n",
      "Loss function {ns, hs, softmax}: loss_name.softmax\n",
      "\n",
      "Hyperparameters search model: \n",
      "Value of the learning rate: 0.27162332716385296\n",
      "Dimensionality of word vectors: 160\n",
      "Number of epochs: 37\n",
      "max length of word ngram: 2\n",
      "Min length of char ngram: 0\n",
      "Max length of char ngram: 0\n",
      "Loss function {ns, hs, softmax}: loss_name.softmax\n"
     ]
    }
   ],
   "source": [
    "print('Default model:')\n",
    "print(\"Value of the learning rate:\", fastText_model.lr)\n",
    "print(\"Dimensionality of word vectors:\", fastText_model.dim)\n",
    "print(\"Number of epochs:\", fastText_model.epoch)\n",
    "print(\"Max length of word ngram :\", fastText_model.wordNgrams)\n",
    "print(\"Min length of char ngram:\", fastText_model.minn)\n",
    "print(\"Max length of char ngram:\", fastText_model.maxn)\n",
    "print(\"Loss function {ns, hs, softmax}:\", fastText_model.loss)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Hyperparameters search model: ')\n",
    "print(\"Value of the learning rate:\", model.lr)\n",
    "print(\"Dimensionality of word vectors:\", model.dim)\n",
    "print(\"Number of epochs:\", model.epoch)\n",
    "print(\"max length of word ngram:\", model.wordNgrams)\n",
    "print(\"Min length of char ngram:\", model.minn)\n",
    "print(\"Max length of char ngram:\", model.maxn)\n",
    "print(\"Loss function {ns, hs, softmax}:\", model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db889f",
   "metadata": {},
   "source": [
    "The 2 models use the same loss function but have several attributes that differ, indeed, the value of the learning rate, of the dimensionality of word vectors and of the number of epochs is higher in the hyperparameters search model.\n",
    "\n",
    "default model:\n",
    "1. Value of the learning rate: 0.1\n",
    "2. Dimensionality of word vectors: 100\n",
    "3. Number of epochs: 5\n",
    "\n",
    "\n",
    "hyperparameters search model: \n",
    "1. Value of the learning rate: 0.27162332716385296\n",
    "2. Dimensionality of word vectors: 160\n",
    "3. Number of epochs: 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba92866",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "\n",
    "The minn and maxn hyperparameters in FastText determine the minimum and maximum character n-gram lengths used during training.\n",
    "If after hyperparameter search on your data, the values of minn and maxn are set to 0, it likely indicates that FastText is not using any character n-grams during training. This can happen if the hyperparameter search determined that the best performing model does not require character n-grams for the specific language and data you are working with.\n",
    "\n",
    "In particular, for languages that do not have a rich morphology, such as English, it is common to find that the optimal values for minn and maxn are set to 0 during hyperparameter search. This is because English words are relatively short and do not require character n-grams to capture meaningful word representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
