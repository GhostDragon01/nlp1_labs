{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8bb0f4",
   "metadata": {},
   "source": [
    " # Introduction to NLP 01\n",
    " # Lab 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14f4ae",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7a4aa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/djulo/.local/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.20.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: jinja2 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/djulo/.local/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3.11/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/djulo/.local/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/djulo/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/djulo/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.7)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/djulo/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/djulo/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/djulo/.local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/djulo/.local/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/djulo/.local/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib64/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djulo/.local/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Large Movie Review Dataset.\\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "from datasets import load_dataset_builder, get_dataset_split_names, load_dataset, get_dataset_config_names\n",
    "ds_builder = load_dataset_builder(\"imdb\")\n",
    "\n",
    "display(ds_builder.info.description, ds_builder.info.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebcdc0a",
   "metadata": {},
   "source": [
    " ### 1. How many splits does the dataset has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c895152a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test', 'unsupervised']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 splits \n"
     ]
    }
   ],
   "source": [
    "splits = get_dataset_split_names(\"imdb\")\n",
    "\n",
    "display(splits)\n",
    "print(f\"We have {len(splits)} splits \") # We have three splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f6845",
   "metadata": {},
   "source": [
    "### 2. How big are these splits ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a28acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 576.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c01934",
   "metadata": {},
   "source": [
    "The train and test split contain 25000 rows each. While the unsupervised split has 50000 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4d174",
   "metadata": {},
   "source": [
    "### 3. What is the proportion of each class on the supervised splits?\n",
    "\n",
    "The proportion of each class on the supervised splits is **1/2**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c89d1",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier\n",
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67fc558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "# Take the training part of the dataset\n",
    "train_data = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d87bc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I rent I be curious yellow from my video store because of all the controversy that surround it when it be first release in 1967 I also hear that at first it be seize by u s custom if it ever try to enter this country therefore be a fan of film consider controversial I really have to see this for myself the plot be center around a young swedish drama student name lena who want to learn everything she can about life in particular she want to focus her attention to make some sort of documentary on what the average swede think about certain political issue such as the vietnam war and race issue in the united states in between ask politician and ordinary denizen of stockholm about their opinion on politic she have sex with her drama teacher classmate and married man what kill I about I be curious yellow be that 40 year ago this be consider pornographic really the sex and nudity scene be few and far between even then it s not shoot like some cheaply make porno while my countryman mind find it shock in reality sex and nudity be a major staple in swedish cinema even ingmar bergman arguably their answer to good old boy john ford have sex scene in his film I do commend the filmmaker for the fact that any sex show in the film be show for artistic purpose rather than just to shock people and make money to be show in pornographic theater in america I be curious yellow be a good film for anyone want to study the meat and potatoe no pun intend of swedish cinema but really this film doesn t have much of a plot',\n",
       " 'I be curious yellow be a risible and pretentious steaming pile it doesn t matter what one s political view be because this film can hardly be take seriously on any level as for the claim that frontal male nudity be an automatic nc 17 that isn t true I ve see r rate film with male nudity grant they only offer some fleeting view but where be the r rate film with gape vulvas and flap labia nowhere because they don t exist the same go for those crappy cable show schlong swinge in the breeze but not a clitoris in sight and those pretentious indie movie like the brown bunny in which we re treat to the site of vincent gallo s throbbing johnson but not a trace of pink visible on chloe sevigny before cry or imply double standard in matter of nudity the mentally obtuse should take into account one unavoidably obvious anatomical difference between man and woman there be no genital on display when actress appear nude and the same can not be say for a man in fact you generally win t see female genital in an american film in anything short of porn or explicit erotica this allege double standard be less a double standard than an admittedly depressing ability to come to term culturally with the inside of woman s body']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the text\n",
    "text = train_data[\"text\"]\n",
    "text[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91ad0cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i rent i be curious yellow from my video store because of all the controversy that surround it when it be first release in 1967 i also hear that at first it be seize by u s custom if it ever try to enter this country therefore be a fan of film consider controversial i really have to see this for myself the plot be center around a young swedish drama student name lena who want to learn everything she can about life in particular she want to focus her attention to make some sort of documentary on what the average swede think about certain political issue such as the vietnam war and race issue in the united states in between ask politician and ordinary denizen of stockholm about their opinion on politic she have sex with her drama teacher classmate and married man what kill i about i be curious yellow be that 40 year ago this be consider pornographic really the sex and nudity scene be few and far between even then it s not shoot like some cheaply make porno while my countryman mind find it shock in reality sex and nudity be a major staple in swedish cinema even ingmar bergman arguably their answer to good old boy john ford have sex scene in his film i do commend the filmmaker for the fact that any sex show in the film be show for artistic purpose rather than just to shock people and make money to be show in pornographic theater in america i be curious yellow be a good film for anyone want to study the meat and potatoe no pun intend of swedish cinema but really this film doesn t have much of a plot',\n",
       " 'i be curious yellow be a risible and pretentious steaming pile it doesn t matter what one s political view be because this film can hardly be take seriously on any level as for the claim that frontal male nudity be an automatic nc 17 that isn t true i ve see r rate film with male nudity grant they only offer some fleeting view but where be the r rate film with gape vulvas and flap labia nowhere because they don t exist the same go for those crappy cable show schlong swinge in the breeze but not a clitoris in sight and those pretentious indie movie like the brown bunny in which we re treat to the site of vincent gallo s throbbing johnson but not a trace of pink visible on chloe sevigny before cry or imply double standard in matter of nudity the mentally obtuse should take into account one unavoidably obvious anatomical difference between man and woman there be no genital on display when actress appear nude and the same can not be say for a man in fact you generally win t see female genital in an american film in anything short of porn or explicit erotica this allege double standard be less a double standard than an admittedly depressing ability to come to term culturally with the inside of woman s body']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "def preprocess(dataset: dict) -> dict:\n",
    "    '''Take a dataset, return the dataset preprocessed'''\n",
    "    \n",
    "    # Convert the text to lowercase and remove <br>\n",
    "    dataset['text'] = dataset['text'].lower().replace('<br /><br />', '')\n",
    "    \n",
    "    # Replace punctuation with spaces\n",
    "    dataset['text'] = re.sub('['+punctuation+']', ' ', dataset['text'])\n",
    "    \n",
    "    # Replace multiple with a single space\n",
    "    dataset['text'] = \" \".join(dataset['text'].split())\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "train_data = train_data.map(preprocess)\n",
    "\n",
    "# Take a look at the preprocessed text\n",
    "train_data['text'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb05277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached processed dataset at /home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-40be9da7ef59e51c.arrow\n"
     ]
    }
   ],
   "source": [
    "# Take the split of test.\n",
    "test_data = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "# Apply preprocessing to the test dataset\n",
    "test_data = test_data.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a89ed0",
   "metadata": {},
   "source": [
    "# 2. Implement own naive Bayes classifier from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab12540c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class MultinomialNaiveBayesClassifier:\n",
    "    '''Define Multinomial Naive Bayes Classifier class'''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Init the logprior, loglikehood and vocabulary for the model'''\n",
    "        self.logprior = None\n",
    "        self.loglikelihood = None\n",
    "        self.vocabulary = None\n",
    "        self.classes = [0, 1]\n",
    "\n",
    "    def train_naive_bayes(self, dataset: dict) -> None:\n",
    "        '''Implementation of the Train Naive Bayes algorithm\n",
    "            Take a dataset and the set of Class C,\n",
    "            Define the logprior, loglikehood and the vocabulary\n",
    "        '''\n",
    "\n",
    "        # D is the list of all paragraphs\n",
    "        documents = dataset['text']\n",
    "\n",
    "        # Number of documents in all the dataset (document = paragraph)\n",
    "        n_doc = len(dataset)\n",
    "\n",
    "        # Initialize logprior as dictionnary\n",
    "        self.logprior = {}\n",
    "\n",
    "        # Initialize bigdoc as dictionnary\n",
    "        # bigdoc contain 2 lists of paragraphs depending on their class\n",
    "        bigdoc = {}\n",
    "\n",
    "        # Vocabulary is the union of all unique words\n",
    "        self.vocabulary = set()\n",
    "        for document in documents:\n",
    "            self.vocabulary.update(document.split())\n",
    "        self.vocabulary = list(self.vocabulary)\n",
    "\n",
    "        # Initialize loglikelihood as dictionnary\n",
    "        self.loglikelihood = {c: {} for c in self.classes}\n",
    "        for c in self.classes:\n",
    "            bigdoc[c] = []\n",
    "\n",
    "            # Number of documents of class c (document = paragraph)\n",
    "            n_class = dataset['label'].count(c)\n",
    "\n",
    "            # Define logprior for class c\n",
    "            self.logprior[c] = math.log((n_class / n_doc))\n",
    "\n",
    "            # For each row of the Dataset\n",
    "            for i in range(len(dataset)):\n",
    "                if dataset[i]['label'] == c:\n",
    "                    # Append the document\n",
    "                    bigdoc[c].append(dataset[i]['text'])\n",
    "\n",
    "            # Return dictionnay with word as keys and occurence of the word as value.\n",
    "            word_counts = Counter(' '.join(bigdoc[c]).split(' '))\n",
    "            total_word_count = sum(word_counts.values())\n",
    "            for word in word_counts:\n",
    "                self.loglikelihood[word, c] = math.log((word_counts[word] + 1) / (total_word_count + len(self.vocabulary)))\n",
    "\n",
    "    def predict(self, testdoc: str) -> int:\n",
    "        '''Implementation of the test naive Bayes algorithm\n",
    "            Take a testing document testdoc and the set of class C\n",
    "            Return a prediction, 0 for negative, 1 for positive'''\n",
    "        \n",
    "        sum = {}\n",
    "        for c in self.classes:\n",
    "            sum[c] = self.logprior[c]\n",
    "            for word in testdoc.split(' '):\n",
    "                if word in self.vocabulary and (word, c) in self.loglikelihood:\n",
    "                    sum[c] += self.loglikelihood[word, c]\n",
    "        return max(sum, key=sum.get)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define a function to apply the predict method to a single example\n",
    "    def predict_all(self, dataset: dict) -> int:\n",
    "        '''Apply Predict method to an entire dataset\n",
    "            return a list of predictions'''\n",
    "        return self.predict(dataset['text'])\n",
    "\n",
    "\n",
    "# Define our model as a Multinomial Naive Bayes Classifier\n",
    "model = MultinomialNaiveBayesClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.train_naive_bayes(train_data)\n",
    "\n",
    "#predictions = list(map(model.predict_example, train_data))\n",
    "#predictions\n",
    "\n",
    "# We load the predictions result because it takes too much time\n",
    "with open('predictions_save', 'r') as f:\n",
    "    content = f.read()\n",
    "    prediction = content.split(', ')\n",
    "    \n",
    "    # convert str to int\n",
    "    prediction = list(map(int, prediction))\n",
    "prediction[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf129a",
   "metadata": {},
   "source": [
    "# 3. Implement a naive Bayes classifier using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e1a7db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Define y_train as the output y\n",
    "y_train = train_data['label']\n",
    "\n",
    "# Apply transformation to the train and test datasets\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Define Multinomial naive bayes model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,y_train)\n",
    "\n",
    "# Predictions of the classifier\n",
    "mnb_prediction = mnb.predict(X_test)\n",
    "\n",
    "print(mnb_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05217e09",
   "metadata": {},
   "source": [
    "# 4. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade319d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn implementation accuracy:  0.8144\n",
      "Our implementation accuracy:  0.68268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Accuracy score with the scikit-learn implementation\n",
    "accuracy_sklearn = accuracy_score(mnb_prediction, test_data['label'])\n",
    "print('Sklearn implementation accuracy: ',accuracy_sklearn)\n",
    "\n",
    "# Accuracy score with our implentation from scratch\n",
    "accuracy_scratch = np.sum([prediction[i] == test_data[i]['label'] for i in range(len(prediction))]) / len(prediction)\n",
    "print('Our implementation accuracy: ',accuracy_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6735e47",
   "metadata": {},
   "source": [
    "# 5. Why the scikit-learn implementation give better results ?\n",
    "\n",
    "The main reason why the scikit-learn implementation give better results is the ability to define hyper-parameters:\n",
    "    \n",
    "1. alpha: Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).\n",
    "2. force_alpha: If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0.\n",
    "3. fit_prior: Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "4. class_prior: Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.\n",
    "\n",
    "    \n",
    "By playing with those hyper-parameters and by choosing the right ones regarding our dataset we can obtain better result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52286d14",
   "metadata": {},
   "source": [
    "# 6. Why is accuracy a sufficient measure of evaluation here?\n",
    "\n",
    "In this model, accuracy is a sufficient measure of evaluation because we are not dealing with imbalanced classes and because here the cost of false positives and false negatives is considered equivalent.\n",
    "\n",
    "In these other cases, other metrics like precision, recall, F1 score, or AUC-ROC could have been more appropriate for evaluating the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154c8b6",
   "metadata": {},
   "source": [
    "# 7. Explaining why the model failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5690e7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blind date columbia pictures 1934 was a decent film but i have a few issues with this film first of all i don t fault the actors in this film at all but more or less i have a problem with the script also i understand that this film was made in the 1930 s and people were looking to escape reality but the script made ann sothern s character look weak she kept going back and forth between suitors and i felt as though she should have stayed with paul kelly s character in the end he truly did care about her and her family and would have done anything for her and he did by giving her up in the end to fickle neil hamilton who in my opinion was only out for a good time paul kelly s character although a workaholic was a man of integrity and truly loved kitty ann sothern as opposed to neil hamilton while he did like her a lot i didn t see the depth of love that he had for her character the production values were great but the script could have used a little work',\n",
       " 'ben rupert grint is a deeply unhappy adolescent the son of his unhappily married parents his father nicholas farrell is a vicar and his mother laura linney is well let s just say she s a somewhat hypocritical soldier in jesus army it s only when he takes a summer job as an assistant to a foul mouthed eccentric once famous and now forgotten actress evie walton julie walters that he finally finds himself in true harold and maude fashion of course evie is deeply unhappy herself and it s only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness of course it s corny and sentimental and very predictable but it has a hard side to it too and walters who could sleep walk her way through this sort of thing if she wanted is excellent it s when she puts the craziness to one side and finds the pathos in the character like hitting the bottle and throwing up in the sink that she s at her best the problem is she s the only interesting character in the film and it s not because of the script which doesn t do anybody any favours grint on the other hand isn t just unhappy he s a bit of a bore as well while linney s starched bitch is completely one dimensional still she s got the english accent off pat the best that can be said for it is that it s mildly enjoyable with the emphasis on the mildly']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for a failed prediction\n",
    "failed_predictions_indexes = [i for i in range(len(mnb_prediction)) if mnb_prediction[i] != test_data[i]['label']]\n",
    "\n",
    "# Take a look at the 2 first documents where the prediction has failed\n",
    "test_data[failed_predictions_indexes[:2]]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42d5ba",
   "metadata": {},
   "source": [
    "We think that the predictions failed for these two examples because there are in these texts the presence of many positive and negative words at the same time. Since the naive Bayes classifier is a naive model, it ignores the position of the words and only takes into account their existence in the text, and also the conditional independence assumption who say that the probabilities P(fi|c) are independent.\n",
    "\n",
    "Those naive assumptions led to a prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b530e",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd34c8",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "### 1.\n",
    "We choose the lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "929d91f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/djulo/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-7469e200be2cf060.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I rent I be curious yellow from my video store because of all the controversy that surround it when it be first release in 1967 I also hear that at first it be seize by u s custom if it ever try to enter this country therefore be a fan of film consider controversial I really have to see this for myself the plot be center around a young swedish drama student name lena who want to learn everything she can about life in particular she want to focus her attention to make some sort of documentary on what the average swede think about certain political issue such as the vietnam war and race issue in the united states in between ask politician and ordinary denizen of stockholm about their opinion on politic she have sex with her drama teacher classmate and married man what kill I about I be curious yellow be that 40 year ago this be consider pornographic really the sex and nudity scene be few and far between even then it s not shoot like some cheaply make porno while my countryman mind find it shock in reality sex and nudity be a major staple in swedish cinema even ingmar bergman arguably their answer to good old boy john ford have sex scene in his film I do commend the filmmaker for the fact that any sex show in the film be show for artistic purpose rather than just to shock people and make money to be show in pornographic theater in america I be curious yellow be a good film for anyone want to study the meat and potatoe no pun intend of swedish cinema but really this film doesn t have much of a plot'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Loading the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_lemming(dataset: dict) -> dict:\n",
    "    '''Apply lemming to the all dataset given in parameter'''\n",
    "    \n",
    "    dataset['text'] = lemming(dataset['text'])\n",
    "    return dataset\n",
    "\n",
    "def lemming(text: str) -> str:\n",
    "    '''Apply lemming on a document'''\n",
    "    \n",
    "    re_word = re.compile(r\"^\\w+$\")\n",
    "    lemmas = [token.lemma_ for token in nlp(text) if re_word.match(token.text)]\n",
    "    return \" \".join(lemmas)\n",
    "   \n",
    "# Apply lemming to the training dataset\n",
    "train_data = train_data.map(preprocess_lemming)    \n",
    "train_data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15607b06",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Train again with the lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a9fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "# Apply lemmatization to the test data\n",
    "test_data = test_data.map(preprocess_lemming)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa2d3bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Define y_train as the output y\n",
    "y_train = train_data['label']\n",
    "\n",
    "# Apply transformoration to the train and test dataset\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Define Multinomial naive bayes model\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train,y_train)\n",
    "\n",
    "# predictions of the classifier\n",
    "mnb_prediction = mnb.predict(X_test)\n",
    "\n",
    "print(mnb_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dae5a1",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3ef69d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80908\n"
     ]
    }
   ],
   "source": [
    "# accuracy score with the scikit-learn implementation\n",
    "accuracy_sklearn = accuracy_score(mnb_prediction, test_data['label'])\n",
    "print(accuracy_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe152124",
   "metadata": {},
   "source": [
    "We can see that the accuracy has decreased after the lemmatization of the dataset text.\n",
    "\n",
    "We can explain that by the fact that we apply lemmatization without stemming and therefore we could have lost the meaning of certain words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
