{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAHHca5l2QRd"
      },
      "source": [
        "# Lab06 - NLP2 - Encoder-decoder model\n",
        "## Using the pyTorch tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA3BfljP19mM"
      },
      "source": [
        "#### install the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIAqcw6r2Ox_",
        "outputId": "0764c172-3dd4-4f66-bb5c-3c876f18e068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (1.26.15)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (16.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "Successfully installed colorama-0.4.6 portalocker-2.7.0 sacrebleu-2.3.1 spacy-3.5.3\n",
            "2023-05-25 17:04:41.729749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-25 17:04:43.171553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-25 17:04:44.464367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-25 17:04:44.464812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-25 17:04:44.464990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-05-25 17:04:57.433689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-25 17:04:58.452572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-25 17:04:59.706395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-25 17:04:59.706848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-25 17:04:59.707023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.2)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy sacrebleu torchdata -U\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeefPbcJ11YO"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w2_g9nG4zMsm"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTIAQR033AHa"
      },
      "source": [
        "Create source and target language tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B9rppaYl13l5"
      },
      "outputs": [],
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgyabLwBYs2d"
      },
      "source": [
        "#### Seq2Seq Network using Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JyogvtWJ3D42"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2GR4K9YxjO"
      },
      "source": [
        "During training, we need a subsequent word mask that will prevent the model from looking into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let’s define a function that will take care of both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eR8fS6rrYn1f"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up_Fvrp-Y879"
      },
      "source": [
        "Let’s now define the parameters of our model and instantiate the same. Below, we also define our loss function which is the cross-entropy loss and the optimizer used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m35_TO28Y6NP"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkTVR9XQZIoz"
      },
      "source": [
        "#### Collation\n",
        "\n",
        "As seen in the Data Sourcing and Processing section, our data iterator yields a pair of raw strings. We need to convert these string pairs into the batched tensors that can be processed by our Seq2Seq network defined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that can be fed directly into our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IkGqx7x0ZCKG"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operationsd\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P88MkAMFZOha"
      },
      "source": [
        "Let’s define training and evaluation loop that will be called for each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qXvxt4zMZL9S"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX0gyBqrZSkA"
      },
      "source": [
        "Now we have all the ingredients to train our model. Let’s do it!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT_FWMLwZQC5",
        "outputId": "002ac055-c564-4bc7-f57c-8cc732b6af61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.344, Val loss: 4.114, Epoch time = 44.405s\n",
            "Epoch: 2, Train loss: 3.760, Val loss: 3.320, Epoch time = 42.185s\n",
            "Epoch: 3, Train loss: 3.161, Val loss: 2.895, Epoch time = 44.152s\n",
            "Epoch: 4, Train loss: 2.768, Val loss: 2.639, Epoch time = 43.888s\n",
            "Epoch: 5, Train loss: 2.480, Val loss: 2.443, Epoch time = 44.197s\n",
            "Epoch: 6, Train loss: 2.251, Val loss: 2.318, Epoch time = 42.843s\n",
            "Epoch: 7, Train loss: 2.061, Val loss: 2.201, Epoch time = 44.234s\n",
            "Epoch: 8, Train loss: 1.897, Val loss: 2.112, Epoch time = 43.028s\n",
            "Epoch: 9, Train loss: 1.754, Val loss: 2.061, Epoch time = 44.232s\n",
            "Epoch: 10, Train loss: 1.631, Val loss: 2.002, Epoch time = 43.423s\n",
            "Epoch: 11, Train loss: 1.524, Val loss: 1.969, Epoch time = 43.072s\n",
            "Epoch: 12, Train loss: 1.419, Val loss: 1.942, Epoch time = 43.713s\n",
            "Epoch: 13, Train loss: 1.334, Val loss: 1.968, Epoch time = 42.927s\n",
            "Epoch: 14, Train loss: 1.252, Val loss: 1.944, Epoch time = 44.026s\n",
            "Epoch: 15, Train loss: 1.173, Val loss: 1.933, Epoch time = 45.058s\n",
            "Epoch: 16, Train loss: 1.103, Val loss: 1.922, Epoch time = 44.695s\n",
            "Epoch: 17, Train loss: 1.039, Val loss: 1.899, Epoch time = 45.538s\n",
            "Epoch: 18, Train loss: 0.979, Val loss: 1.906, Epoch time = 44.747s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1xwH_NmbZVZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e304a36-b9f2-4804-bb9f-c1e70e6de4ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people standing in front of an igloo . \n"
          ]
        }
      ],
      "source": [
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxCwnQU4Zy5s"
      },
      "source": [
        "## Theoretical questions\n",
        "\n",
        "##### **1.In the positional encoding, why are we using a combination of sinus and cosinus?**\n",
        "\n",
        "In positional encoding, a combination of sine and cosine functions is used to represent the position of each word in the input sequence. This is done to capture both the relative and absolute positions of the words. The sinusoidal functions provide different frequencies that encode different positions along the sequence. By using a combination of sine and cosine functions, the positional encoding can represent different positions with unique patterns, allowing the model to differentiate between words at different positions in the sequence.\n",
        "\n",
        "##### **2. In the Seq2SeqTransformer class, What is the parameter nhead for? What is the point of the generator?**\n",
        "\n",
        "- The parameter \"nhead\" stands for the number of attention heads. In the transformer model, multi-head attention is used to capture different types of dependencies between words in the input sequence. Each attention head attends to different parts of the sequence and learns different patterns of relationships. Increasing the number of attention heads allows the model to capture more complex dependencies and enhance its ability to focus on different aspects of the input during attention calculation.\n",
        "\n",
        "- The \"generator\" is responsible for generating the output sequence based on the decoder's hidden state. It takes the decoder's hidden state as input and applies a linear transformation followed by a softmax activation to produce the probability distribution over the vocabulary. The generator essentially predicts the next word in the output sequence.\n",
        "\n",
        "\n",
        "##### **3.Describe the goal of the create_mask function. Why does it handle differently the source and target masks?**\n",
        "\n",
        "The goal of the create_mask function is to create attention masks for the source and target sequences in the transformer model. The masks are used during the attention calculation to ensure that the model attends only to the relevant parts of the sequences.\n",
        "The function handles the source and target masks differently because they serve different purposes in the model:\n",
        "\n",
        "- Source mask: The source mask is used in the encoder to prevent attending to future positions in the source sequence.This ensures that the encoder only attends to the positions that have been already processed and avoids any information leakage from future positions.\n",
        "\n",
        "- Target mask: The target mask is used in the decoder during both the self-attention and encoder-decoder attention calculations. It serves two purposes. Firstly, it prevents attending to future positions, similar to the source mask. Secondly, it also masks out the padding positions in the target sequence so that the model does not attend to them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsDK7ce5fMln"
      },
      "source": [
        "## Decoding functions\n",
        "\n",
        "- A top-k sampling with temperature for decoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YpXXi04UZYf0"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def topk_sample(logits: Tensor, k: int, temperature: float) -> Tensor:\n",
        "    \"\"\"\n",
        "    Perform top-k sampling with temperature on the logits.\n",
        "\n",
        "    Args:\n",
        "        logits (Tensor): Logits from the model output.\n",
        "        k (int): Number of candidates to consider.\n",
        "        temperature (float): Temperature value for scaling logits.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Top-k value.\n",
        "\n",
        "    \"\"\"\n",
        "    scaled_logits = logits / temperature\n",
        "    topk_values, topk_indices = torch.topk(scaled_logits, k=k, dim=-1)\n",
        "    probabilities = F.softmax(topk_values, dim=-1)\n",
        "    sampled_token = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "    next_token = topk_indices[:, sampled_token].squeeze()\n",
        "\n",
        "    return next_token\n",
        "\n",
        "\n",
        "def topk_temperature_decode(\n",
        "    model,\n",
        "    src: Tensor,\n",
        "    src_mask: Tensor,\n",
        "    max_len: int,\n",
        "    start_symbol: int,\n",
        "    k: int,\n",
        "    temperature: float\n",
        ") -> Tensor:\n",
        "  \"\"\"\n",
        "    Generate output sequence using greedy algorithm with top-k sampling and temperature.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        src (Tensor): Source input.\n",
        "        src_mask (Tensor): Source input mask.\n",
        "        max_len (int): Maximum length of the output sequence.\n",
        "        start_symbol (int): Start symbol for decoding.\n",
        "        k (int): Number of candidates to consider for top-k sampling.\n",
        "        temperature (float): Temperature value for scaling logits during sampling.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Generated output sequence.\n",
        "\n",
        "    \"\"\"\n",
        " \n",
        "  src = src.to(DEVICE)\n",
        "  src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "  memory = model.encode(src, src_mask)\n",
        "  ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "  for i in range(max_len-1):\n",
        "      memory = memory.to(DEVICE)\n",
        "      tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                  .type(torch.bool)).to(DEVICE)\n",
        "      out = model.decode(ys, memory, tgt_mask)\n",
        "      out = out.transpose(0, 1)\n",
        "      prob = model.generator(out[:, -1])\n",
        "      next_word = topk_sample(prob, k=k, temperature=temperature)\n",
        "      ys = torch.cat([ys,\n",
        "                      torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "      if next_word == EOS_IDX:\n",
        "          break\n",
        "  return ys\n",
        "\n",
        "def translate_topk_temperature(\n",
        "    model: nn.Module,\n",
        "    src_sentence: str,\n",
        "    k: int,\n",
        "    temperature: float\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Translate an input sentence into the target language using top k temperature decoding\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained translation model.\n",
        "        src_sentence (str): Input sentence to be translated.\n",
        "        k (int): Number of candidates to consider for top-k sampling.\n",
        "        temperature (float): Temperature value for scaling logits during sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: Translated sentence in the target language.\n",
        "\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = topk_temperature_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k=k, temperature=temperature).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQWLkIsgkmmG"
      },
      "source": [
        "- A top-p sampling with temperature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8YUqPe8yknPF"
      },
      "outputs": [],
      "source": [
        "def top_p_sample(logits: Tensor, p: float, temperature: float) -> Tensor:\n",
        "    \"\"\"\n",
        "    Perform top-p sampling with temperature on the logits.\n",
        "\n",
        "    Args:\n",
        "        logits (Tensor): Logits from the model output.\n",
        "        p (float): Cumulative probability threshold.\n",
        "        temperature (float): Temperature value for scaling logits.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Sampled token.\n",
        "\n",
        "    \"\"\"\n",
        "    scaled_logits = logits / temperature\n",
        "    probs = F.softmax(scaled_logits, dim=-1)\n",
        "    sorted_probs, indices = torch.sort(probs, dim=-1, descending=True)\n",
        "    cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    nucleus = cum_sum_probs < p\n",
        "    nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1)\n",
        "    sorted_log_probs = torch.log(sorted_probs)\n",
        "\n",
        "    sorted_log_probs[~nucleus] = float('-inf')\n",
        "    sampled_indices = indices[nucleus]\n",
        "    sampled_token = torch.multinomial(sorted_log_probs.exp(), num_samples=1)\n",
        "    next_token = sampled_indices[sampled_token].squeeze()\n",
        "    return next_token\n",
        "\n",
        "def topp_temperature_decode(\n",
        "    model,\n",
        "    src: Tensor,\n",
        "    src_mask: Tensor,\n",
        "    max_len: int,\n",
        "    start_symbol: int,\n",
        "    p: float,\n",
        "    temperature: float,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Generate output sequence using greedy algorithm with top-p sampling and temperature.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        src (Tensor): Source input.\n",
        "        src_mask (Tensor): Source input mask.\n",
        "        max_len (int): Maximum length of the output sequence.\n",
        "        start_symbol (int): Start symbol for decoding.\n",
        "        p (float): Cumulative probability threshold for top-p sampling.\n",
        "        temperature (float): Temperature value for scaling logits during sampling.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Generated output sequence.\n",
        "\n",
        "    \"\"\"\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (\n",
        "            generate_square_subsequent_mask(ys.size(0))\n",
        "            .type(torch.bool)\n",
        "            .to(DEVICE)\n",
        "        )\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        next_word = top_p_sample(prob, p=p, temperature=temperature)\n",
        "        ys = torch.cat(\n",
        "            [ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0\n",
        "        )\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return ys\n",
        "\n",
        "\n",
        "def translate_topp_temperature(\n",
        "    model: nn.Module,\n",
        "    src_sentence: str,\n",
        "    p: int,\n",
        "    temperature: float\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Translate an input sentence into the target language using top k temperature decoding\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained translation model.\n",
        "        src_sentence (str): Input sentence to be translated.\n",
        "        k (int): Number of candidates to consider for top-k sampling.\n",
        "        temperature (float): Temperature value for scaling logits during sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: Translated sentence in the target language.\n",
        "\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = topp_temperature_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, p=p, temperature=temperature).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le9PqwGKlV1R"
      },
      "source": [
        "### Compare Translation by playing with the k, p and temperature parameters\n",
        "\n",
        "We know that: \n",
        "\n",
        "- The **k parameter** determines the number of candidates considered for sampling. It controls the size of the set of words from which the final word is sampled.\n",
        "\n",
        "- The **temperature parameter** controls the softmax temperature during sampling.\n",
        "\n",
        "- The **p parameter** determines the cumulative probability threshold for top-p sampling. It controls the size of the set of words considered for sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Wzdn4jvildHE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3540f981-f0c0-4e5c-a09e-18c57e0141d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP K DECODER:\n",
            "case k=3, temperature=0.6 ->  A group of people standing in front of an auditorium . \n",
            "case k=3, temperature=0.8 ->  A group of people stand in front of an igloo . \n",
            "case k=3, temperature=1.0 ->  A group of people standing in front an igloo . \n",
            "case k=3, temperature=1.2 ->  A group of people standing in front of an abandoned . \n",
            "case k=5, temperature=0.6 ->  A group of people standing in front of an igloo \n",
            "case k=5, temperature=0.8 ->  A group of people stand in front an igloo . \n",
            "case k=5, temperature=1.0 ->  A group of people standing in front an auditorium . \n",
            "case k=5, temperature=1.2 ->  A group of people standing in front of an igloo . \n",
            "case k=10, temperature=0.6 ->  A group of people stand in front of an igloo . \n",
            "case k=10, temperature=0.8 ->  A group of people stand in front of an ATM . \n",
            "case k=10, temperature=1.0 ->  A group of people stand in front of an igloo . \n",
            "case k=10, temperature=1.2 ->  A group of people stand in front of an abandoned . \n",
            "case k=50, temperature=0.6 ->  A group of people stand in front of an igloo . \n",
            "case k=50, temperature=0.8 ->  A group of people stand in front of an acrobatic SquarePants . \n",
            "case k=50, temperature=1.0 ->  At an operation , a group of Middle machines . \n",
            "case k=50, temperature=1.2 ->  A group of people stand in front of an obnoxiously anvil . \n",
            "\n",
            "TOP P DECODER:\n",
            "case p=0.1, temperature=0.6 ->  A group of people standing in front of an igloo . \n",
            "case p=0.1, temperature=0.8 ->  A group of people standing in front of an igloo . \n",
            "case p=0.1, temperature=1.0 ->  A group of people standing in front of an abandoned machines . \n",
            "case p=0.1, temperature=1.2 ->  A group of people standing in front of an artistic aircrew setting . \n",
            "case p=0.3, temperature=0.6 ->  A group of people standing in front of an igloo . \n",
            "case p=0.3, temperature=0.8 ->  A group of people standing in front of an igloo . \n",
            "case p=0.3, temperature=1.0 ->  A group of people standing in front of an arbor . \n",
            "case p=0.3, temperature=1.2 ->  A group of people standing in front of an elaborately . \n",
            "case p=0.8, temperature=0.6 ->  A group of people stand in front of an Steakhouse . \n",
            "case p=0.8, temperature=0.8 ->  A group of people stand in front of an oversize Thomas . \n",
            "case p=0.8, temperature=1.0 ->  A group of people stand in front of an helm . \n",
            "case p=0.8, temperature=1.2 ->  A group of people stand in front of an arbour . \n",
            "case p=0.9, temperature=0.6 ->  A group of people stand in front of an igloo . \n",
            "case p=0.9, temperature=0.8 ->  A group of people standing in front of an entrances . \n",
            "case p=0.9, temperature=1.0 ->  A group of people stand in front of an daisies . \n",
            "case p=0.9, temperature=1.2 ->  A group of people standing in front of an bushy assignment . \n"
          ]
        }
      ],
      "source": [
        "k_s = [3, 5, 10, 50]\n",
        "p_s = [0.1, 0.3, 0.8, 0.9]\n",
        "temperatures = [0.6, 0.8, 1.0, 1.2]\n",
        "\n",
        "print(\"TOP K DECODER:\")\n",
        "for k in k_s:\n",
        "  for temperature in temperatures:\n",
        "    translation = translate_topk_temperature(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k, temperature)\n",
        "    print(f\"case k={k}, temperature={temperature} -> {translation}\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"TOP P DECODER:\")\n",
        "for p in p_s:\n",
        "  for temperature in temperatures:\n",
        "    translation = translate_topp_temperature(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p, temperature)\n",
        "    print(f\"case p={p}, temperature={temperature} -> {translation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3UOJsLFPQ0t"
      },
      "source": [
        "According to the result we can say that:\n",
        "\n",
        "- A **larger value of k** includes more candidates, increasing the diversity of potential samples. A **smaller value of k** restricts the candidates to a smaller subset, leading to more deterministic samples\n",
        "\n",
        "-  A **higher temperature (>1)** softens the probability distribution, making lower probability words more likely to be sampled. This introduces more randomness and diversity. Conversely, a **lower temperature (<1)** sharpens the distribution, making high probability words more likely to be sampled, reducing randomness.\n",
        "\n",
        "-  A **larger value of p** includes a larger portion of the probability mass of the probability distribution, resulting in a more diverse sample. A **smaller value of p** focuses on a narrower set of high-probability words, leading to more focused and deterministic samples.\n",
        "\n",
        "- **Greedy decoding** always get the word with the highest probability. For this reason, leading to deterministic samples.\n",
        "\n",
        "\n",
        "\n",
        "#### To conclude:\n",
        "\n",
        "The choice of k (topk case) or p (topp case) and temperature depends on the specific task and desired outcomes. Here are some considerations:\n",
        "\n",
        "- For **more diverse and exploratory outputs**, higher values of k (topk case) or p (topp case) and higher values of temperature can be used.\n",
        "\n",
        "- If you want **more controlled and focused outputs**, lower values of k (topk case) or p (topp case) and lower values of temperature can be used.\n",
        "\n",
        "The challenge is to experiment with different combinations of k (topk case) or p (topp case) and temperature to find the right balance between diversity and control in the generated samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_M9L84RVO1h"
      },
      "source": [
        "## Compute the BLEU score of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zMsPeAGzTRlQ"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = Multi30k(root='.data', split=('train', 'valid', 'test'), language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# Define parameters for topk and topp samplings\n",
        "temperature=0.8\n",
        "p = 1.0\n",
        "k = 3\n",
        "\n",
        "# Evaluate the model using sacreBLEU\n",
        "greedy_translated_outputs = []\n",
        "topk_translated_outputs = []\n",
        "topp_translated_outputs = []\n",
        "reference_translations = []\n",
        "\n",
        "for example in valid_dataset:\n",
        "    src_sentence = example[0]\n",
        "    tgt_sentence = example[1]\n",
        "\n",
        "    # Add greedy translation\n",
        "    greedy_translated_outputs.append(translate(transformer, src_sentence))\n",
        "\n",
        "    # Add topk translation\n",
        "    topk_translated_outputs.append(translate_topk_temperature(transformer, src_sentence, k, temperature))\n",
        "\n",
        "    # Add topp translation\n",
        "    topp_translated_outputs.append(translate_topp_temperature(transformer, src_sentence, p, temperature))\n",
        "\n",
        "    # Add expected translation\n",
        "    reference_translations.append([tgt_sentence])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = BLEU()\n",
        "\n",
        "# Getting score for greedy decoding\n",
        "greedy_bleu = bleu.corpus_score(greedy_translated_outputs, reference_translations)\n",
        "\n",
        "# Print the output values for greedy decoding\n",
        "print(\"GREEDY DECODING:\")\n",
        "print(\"BLEU Score:\", greedy_bleu)\n",
        "print()\n",
        "\n",
        "# Getting score for topk decoding\n",
        "topk_bleu = bleu.corpus_score(topk_translated_outputs, reference_translations)\n",
        "\n",
        "# Print the output values for greedy decoding\n",
        "print(\"top-k DECODING:\")\n",
        "print(\"BLEU Score:\", topk_bleu)\n",
        "print()\n",
        "\n",
        "# Getting score for topp decoding\n",
        "topp_bleu = bleu.corpus_score(topp_translated_outputs, reference_translations)\n",
        "\n",
        "# Print the output values for greedy decoding\n",
        "print(\"top-p DECODING:\")\n",
        "print(\"BLEU Score:\", topp_bleu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV1AfQUSaXsW",
        "outputId": "bc21bbc5-dc1c-471d-bf59-565ab2f39416"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GREEDY DECODING:\n",
            "BLEU Score: BLEU = 44.18 100.0/57.1/33.3/20.0 (BP = 1.000 ratio = 1.000 hyp_len = 8 ref_len = 8)\n",
            "\n",
            "top-k DECODING:\n",
            "BLEU Score: BLEU = 44.18 100.0/57.1/33.3/20.0 (BP = 1.000 ratio = 1.000 hyp_len = 8 ref_len = 8)\n",
            "\n",
            "top-p DECODING:\n",
            "BLEU Score: BLEU = 38.26 90.0/66.7/25.0/14.3 (BP = 1.000 ratio = 1.000 hyp_len = 10 ref_len = 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the breakdown of each result, respectively we will talk about the values ​​in the following form (greedy_result, topk_result, topp_result):\n",
        "\n",
        "- BLEU: The overall BLEU score obtained, which are (44.18, 44.18, 38.26) in this case. BLEU scores range from 0 to 100, with higher scores indicating better translation quality.\n",
        "\n",
        "- (100, 100, 90): This value represents the percentage of 1-gram matches between the system translation and the reference translations. It indicates how well the system's unigram (single word) choices align with the reference translations.\n",
        "\n",
        "- (57.1, 57.1, 66.7): This value represents the percentage of 2-gram matches between the system translation and the reference translations. It measures how well the system's bigram (pair of consecutive words) choices align with the reference translations.\n",
        "\n",
        "- (33.3, 33.3, 25): This value represents the percentage of 3-gram matches between the system translation and the reference translations. It evaluates how well the system's trigram (sequence of three consecutive words) choices align with the reference translations.\n",
        "\n",
        "- (20, 20, 14.3): This value represents the percentage of 4-gram matches between the system translation and the reference translations. It assesses how well the system's four-gram (sequence of four consecutive words) choices align with the reference translations.\n",
        "\n",
        "- BP (Brevity Penalty): The brevity penalty factor applied to the BLEU score. In this case, BP is always 1.000, which indicates that the system's translation length is slightly shorter than the average reference translation length.\n",
        "\n",
        "- Ratio: The ratio of the system's translation length to the average reference translation length. In this case, the ratio are always 100.0, suggesting that the system's translation length is equal to the average reference translation length.\n",
        "\n",
        "- hyp_len: The length of the system's translation, which are (8, 8, 10) in this case.The hyp_len are always 100.0, suggesting that the system's translation length is equal to the average reference translation length.\n",
        "\n",
        "- ref_len: The average length of the reference translations, which are (8, 8, 10) in this case."
      ],
      "metadata": {
        "id": "SsBgQWaP9G3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning parameters\n",
        "#### For top-k sampling"
      ],
      "metadata": {
        "id": "W47HVXHQjg_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter search space\n",
        "temperatures = [0.8, 0.9, 1.0]\n",
        "k_values = [5, 10, 20]\n",
        "\n",
        "best_score = 0.0\n",
        "best_params = {}\n",
        "\n",
        "# Perform hyperparameter search\n",
        "for temperature in temperatures:\n",
        "  for k in k_values:\n",
        "    translated_outputs = []\n",
        "    reference_translations = []\n",
        "    for example in test_dataset:\n",
        "        src_sentence = example[0]\n",
        "        tgt_sentence = example[1]\n",
        "        \n",
        "        # Generate the translation with specified hyperparameters\n",
        "        translation = translate_topk_temperature(transformer, src_sentence, k, temperature)\n",
        "        translated_outputs.append(translation)\n",
        "        reference_translations.append([tgt_sentence])\n",
        "\n",
        "    # Compute sacreBLEU score\n",
        "    bleu = sacrebleu.corpus_bleu(translated_outputs, reference_translations)\n",
        "\n",
        "    # Print the hyperparameters and corresponding BLEU score\n",
        "    print(\"Temperature:\", temperature)\n",
        "    print(\"k:\", k)\n",
        "    print(\"BLEU Score:\", bleu.score)\n",
        "    print()\n",
        "\n",
        "    # Update best score and best parameters if a higher score is achieved\n",
        "    if bleu.score > best_score:\n",
        "        best_score = bleu.score\n",
        "        best_params = {\n",
        "            'temperature': temperature,\n",
        "            'k': k,\n",
        "        }\n",
        "\n",
        "# Print the best parameters and corresponding BLEU score\n",
        "print(\"Best Parameters:\")\n",
        "print(\"Temperature:\", best_params['temperature'])\n",
        "print(\"k:\", best_params['k'])\n",
        "print(\"Best BLEU Score:\", best_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T0OqSKTcxmi",
        "outputId": "ecadd930-0868-486d-9b26-ce1d4f4e9999"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature: 0.8\n",
            "k: 5\n",
            "BLEU Score: 44.833867003844574\n",
            "\n",
            "Temperature: 0.8\n",
            "k: 10\n",
            "BLEU Score: 54.60241725418134\n",
            "\n",
            "Temperature: 0.8\n",
            "k: 20\n",
            "BLEU Score: 49.338853632819\n",
            "\n",
            "Temperature: 0.9\n",
            "k: 5\n",
            "BLEU Score: 46.92470064105599\n",
            "\n",
            "Temperature: 0.9\n",
            "k: 10\n",
            "BLEU Score: 49.616830003403614\n",
            "\n",
            "Temperature: 0.9\n",
            "k: 20\n",
            "BLEU Score: 47.7189707581088\n",
            "\n",
            "Temperature: 1.0\n",
            "k: 5\n",
            "BLEU Score: 46.92470064105601\n",
            "\n",
            "Temperature: 1.0\n",
            "k: 10\n",
            "BLEU Score: 32.091389827941\n",
            "\n",
            "Temperature: 1.0\n",
            "k: 20\n",
            "BLEU Score: 48.326978309062206\n",
            "\n",
            "Best Parameters:\n",
            "Temperature: 0.8\n",
            "k: 10\n",
            "Best BLEU Score: 54.60241725418134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter search space\n",
        "p_values = [0.8, 0.9, 1.0]\n",
        "\n",
        "best_score = 0.0\n",
        "best_params = {}\n",
        "\n",
        "# Perform hyperparameter search\n",
        "for temperature in temperatures:\n",
        "  for p in p_values:\n",
        "    translated_outputs = []\n",
        "    reference_translations = []\n",
        "    for example in test_dataset:\n",
        "        src_sentence = example[0]\n",
        "        tgt_sentence = example[1]\n",
        "        \n",
        "        # Generate the translation with specified hyperparameters\n",
        "        translation = translate_topp_temperature(transformer, src_sentence, p, temperature)\n",
        "        translated_outputs.append(translation)\n",
        "        reference_translations.append([tgt_sentence])\n",
        "\n",
        "    # Compute sacreBLEU score\n",
        "    bleu = sacrebleu.corpus_bleu(translated_outputs, reference_translations)\n",
        "\n",
        "    # Print the hyperparameters and corresponding BLEU score\n",
        "    print(\"Temperature:\", temperature)\n",
        "    print(\"p:\", p)\n",
        "    print(\"BLEU Score:\", bleu.score)\n",
        "    print()\n",
        "\n",
        "    # Update best score and best parameters if a higher score is achieved\n",
        "    if bleu.score > best_score:\n",
        "        best_score = bleu.score\n",
        "        best_params = {\n",
        "            'temperature': temperature,\n",
        "            'p': p,\n",
        "        }\n",
        "\n",
        "# Print the best parameters and corresponding BLEU score\n",
        "print(\"Best Parameters:\")\n",
        "print(\"Temperature:\", best_params['temperature'])\n",
        "print(\"p:\", best_params['p'])\n",
        "print(\"Best BLEU Score:\", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERLLTlfDkYsk",
        "outputId": "9726b2aa-29a9-49b5-ce4d-9d6860d66457"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature: 0.8\n",
            "p: 0.8\n",
            "BLEU Score: 60.427507947135354\n",
            "\n",
            "Temperature: 0.8\n",
            "p: 0.9\n",
            "BLEU Score: 57.21248424548516\n",
            "\n",
            "Temperature: 0.8\n",
            "p: 1.0\n",
            "BLEU Score: 48.326978309062206\n",
            "\n",
            "Temperature: 0.9\n",
            "p: 0.8\n",
            "BLEU Score: 39.34995962231127\n",
            "\n",
            "Temperature: 0.9\n",
            "p: 0.9\n",
            "BLEU Score: 53.3167536340577\n",
            "\n",
            "Temperature: 0.9\n",
            "p: 1.0\n",
            "BLEU Score: 54.10822690539397\n",
            "\n",
            "Temperature: 1.0\n",
            "p: 0.8\n",
            "BLEU Score: 41.882168504198276\n",
            "\n",
            "Temperature: 1.0\n",
            "p: 0.9\n",
            "BLEU Score: 36.55552228545123\n",
            "\n",
            "Temperature: 1.0\n",
            "p: 1.0\n",
            "BLEU Score: 53.48259312838876\n",
            "\n",
            "Best Parameters:\n",
            "Temperature: 0.8\n",
            "p: 0.8\n",
            "Best BLEU Score: 60.427507947135354\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}