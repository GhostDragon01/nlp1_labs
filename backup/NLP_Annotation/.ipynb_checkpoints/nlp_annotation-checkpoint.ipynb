{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6378e551",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "#### Choice of the dataset\n",
    "\n",
    "We have chosen to use the **offensive dataset** because the **hate dataset** has a **CC BY-NC 4.0 license**, which does **not authorize** its use for **commercial purposes**. \n",
    "\n",
    "# Evaluating the dataset\n",
    "\n",
    "## 1. Dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105986e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0db157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (C:/Users/DELL/.cache/huggingface/datasets/tweet_eval/offensive/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e0c84205ad443cba590cc4e4ad7c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweet_dataset = load_dataset(\"tweet_eval\",'offensive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43bae25",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset split :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d656233a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 11916\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 860\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1324\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0162e1",
   "metadata": {},
   "source": [
    "The dataset contains **14100 labelled texts** distributed as follows :\n",
    "- **`Train set`** : **85%** of the dataset\n",
    "- **`Test set`** : **6%** of the dataset\n",
    "- **`Validation set`** : **9%** of the dataset\n",
    "\n",
    "For offensive config:\n",
    "\n",
    "- **`Text`** : a string feature containing the tweet.\n",
    "\n",
    "- **`Label`** : an int classification label with the following mapping: **0: non-offensive**, **1: offensive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6036413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "Non-offensive:  7975 , Offensive :  3941\n",
      "Test dataset:\n",
      "Non-offensive:  620 , Offensive :  240\n",
      "Validation dataset:\n",
      "Non-offensive:  865 , Offensive :  459\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset:\")\n",
    "print(\"Non-offensive: \", tweet_dataset['train']['label'].count(0),\n",
    "      \", Offensive : \", tweet_dataset['train']['label'].count(1))\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "print(\"Non-offensive: \", tweet_dataset['test']['label'].count(0),\n",
    "      \", Offensive : \", tweet_dataset['test']['label'].count(1))\n",
    "\n",
    "print(\"Validation dataset:\")\n",
    "print(\"Non-offensive: \", tweet_dataset['validation']['label'].count(0),\n",
    "      \", Offensive : \", tweet_dataset['validation']['label'].count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddacbc",
   "metadata": {},
   "source": [
    "Looking at the data, we can see that there is more non-offensive data than offensive one. The data is therefore not balanced between the two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c91a1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"__init__.pxd\", line 942, in numpy.import_array\n",
      "RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_24592\\608258965.py\", line 1, in <module>\n",
      "    import bertopic\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bertopic\\__init__.py\", line 1, in <module>\n",
      "    from bertopic._bertopic import BERTopic\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\bertopic\\_bertopic.py\", line 37, in <module>\n",
      "    import hdbscan\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\hdbscan\\__init__.py\", line 1, in <module>\n",
      "    from .hdbscan_ import HDBSCAN, hdbscan\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\hdbscan\\hdbscan_.py\", line 20, in <module>\n",
      "    from ._hdbscan_linkage import (\n",
      "  File \"hdbscan\\_hdbscan_linkage.pyx\", line 1, in init hdbscan._hdbscan_linkage\n",
      "  File \"hdbscan\\dist_metrics.pyx\", line 13, in init hdbscan.dist_metrics\n",
      "  File \"__init__.pxd\", line 944, in numpy.import_array\n",
      "ImportError: numpy.core.multiarray failed to import\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1049, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 935, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1003, in get_records\n",
      "    lines, first = inspect.getsourcelines(etb.tb_frame)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\inspect.py\", line 1121, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\inspect.py\", line 958, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "import bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tweet_dataset[\"train\"]\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd1dc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
